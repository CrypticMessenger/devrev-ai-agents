{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:c:\\Users\\Pratham\\Desktop\\dir\\InterIIT\\DevRev\\devrev-ai-agents\\palm_subtask_responses\\etc\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "from time import sleep\n",
    "import google.generativeai as palm\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google.auth.transport.requests import Request\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "cwd = Path.cwd().joinpath(\"etc\")\n",
    "logging.info(cwd)\n",
    "\n",
    "SCOPES = [\"https://www.googleapis.com/auth/generative-language.tuning\"]\n",
    "\n",
    "\n",
    "def load_creds(token_path=\"token.json\", client_secret_path=\"client_secret.json\"):\n",
    "    \"\"\"Converts `oauth-client-id.json` to a credential object.\n",
    "\n",
    "    This function caches the generated tokens to minimize the use of the\n",
    "    consent screen.\n",
    "    \"\"\"\n",
    "    creds = None\n",
    "    # The file token.json stores the user's access and refresh tokens, and is\n",
    "    # created automatically when the authorization flow completes for the first\n",
    "    # time.\n",
    "    if os.path.exists(token_path):\n",
    "        creds = Credentials.from_authorized_user_file(token_path, SCOPES)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(client_secret_path, SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open(token_path, \"w\") as token:\n",
    "            token.write(creds.to_json())\n",
    "    return creds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using text model: models/text-bison-001\n",
      "INFO:root:Using argument mapping model: tunedModels/argumentmappingmodel3-tyruaig3jv4g\n",
      "INFO:root:Using embeddings model: models/embedding-gecko-001\n"
     ]
    }
   ],
   "source": [
    "creds = load_creds(\n",
    "    token_path=cwd.joinpath(\"token.json\"),\n",
    "    client_secret_path=cwd.joinpath(\"ft_model_secret.json\"),\n",
    ")\n",
    "\n",
    "# Configure PALM and fetch models\n",
    "palm.configure(credentials=creds)\n",
    "\n",
    "text_model = [\n",
    "    m for m in palm.list_models() if \"generateText\" in m.supported_generation_methods\n",
    "][0].name\n",
    "\n",
    "argument_mapping_model = [\n",
    "    m for m in palm.list_tuned_models() if \"argumentmappingmodel3\" in m.name\n",
    "][0].name\n",
    "\n",
    "embeddings_model = [\n",
    "    m for m in palm.list_models() if \"embedText\" in m.supported_generation_methods\n",
    "][0].name\n",
    "\n",
    "logging.info(f\"Using text model: {text_model}\")\n",
    "logging.info(f\"Using argument mapping model: {argument_mapping_model}\")\n",
    "logging.info(f\"Using embeddings model: {embeddings_model}\")\n",
    "\n",
    "constants = json.load(open(cwd.joinpath(\"constants.json\")))\n",
    "tools = constants[\"tools\"]\n",
    "example_prompts = constants[\"examples\"]\n",
    "arguments_descripiton = json.load(open(cwd.joinpath(\"refined_arguments_description.json\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_argument_descriptions(tools, look_in=None):\n",
    "    \"\"\"\n",
    "    Invokes the LLM (One-time call) to generate argument descriptions for each tool\n",
    "    and saves them in a json file.\n",
    "    \"\"\"\n",
    "    arg_prompt = constants[\"argument_processing_prompt\"]\n",
    "    if look_in is not None and os.path.exists(look_in):\n",
    "        arguments_description = json.load(open(look_in, \"r\"))\n",
    "    else:\n",
    "        for tool in tools:\n",
    "            arguments_description[tool[\"name\"]] = []\n",
    "            for argument in tool.get(\"arguments\", []):\n",
    "                output = palm.generate_text(\n",
    "                    model=text_model,\n",
    "                    prompt=arg_prompt % argument,\n",
    "                    temperature=0,\n",
    "                    max_output_tokens=800,\n",
    "                )\n",
    "                arguments_description[tool[\"name\"]].append(\n",
    "                    {argument[\"name\"]: eval(output.result)}\n",
    "                )\n",
    "        if look_in is not None:\n",
    "            json.dump(arguments_description, open(look_in, \"w\"))\n",
    "    return arguments_description\n",
    "\n",
    "\n",
    "def get_tools_description(tools, argument_descriptions):\n",
    "    \"\"\"\n",
    "    Generates a description of all tools and their arguments.\n",
    "    \"\"\"\n",
    "    tools_description = \"\"\n",
    "    for tool in tools:\n",
    "        tools_description += (\n",
    "            \"\\n\" + f\"{tool['name']}:{tool['description'].split('.')[0]}\"\n",
    "        )\n",
    "        for argument in argument_descriptions[tool[\"name\"]]:\n",
    "            tools_description += \" with args:\"\n",
    "            for arg, props in argument.items():\n",
    "                tools_description += f\"\\n\\t{arg}:{props['desc'].split('.')[0]}\"\n",
    "    return tools_description\n",
    "\n",
    "\n",
    "def segement_task(task_statement: str):\n",
    "    \"\"\"\n",
    "    Given a task statement, segments it into subtasks and performs coreference resolution\n",
    "    \"\"\"\n",
    "    segmentation_prompt = constants[\"segmentation_prompt\"]\n",
    "\n",
    "    response = palm.generate_text(\n",
    "        model=text_model,\n",
    "        prompt=segmentation_prompt % task_statement,\n",
    "        temperature=0,\n",
    "        max_output_tokens=800,\n",
    "    )\n",
    "\n",
    "    return eval(response.result)\n",
    "\n",
    "\n",
    "def get_tools_for_tasks(tasks, tools_description):\n",
    "    \"\"\"\n",
    "    It takes a list of tasks and a description of all tools and their arguments\n",
    "    and returns a list of tuples of the form (task, tool) where tool is the most\n",
    "    relevant tool for the given task.\n",
    "\n",
    "    (Invokes LLM)\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    tool_getter_prompt = constants[\"tool_getter_prompt\"]\n",
    "    for task in tasks:\n",
    "        response = palm.generate_text(\n",
    "            model=text_model,\n",
    "            prompt=tool_getter_prompt % (tools_description, task),\n",
    "            temperature=0,\n",
    "            max_output_tokens=800,\n",
    "        )\n",
    "        result = response.result\n",
    "        if result == \"None\":\n",
    "            return []\n",
    "        output.append((task, response.result))\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_relevant_tools(tasks, tools, argument_descriptions):\n",
    "    tools_description = get_tools_description(tools, argument_descriptions)\n",
    "    return get_tools_for_tasks(tasks, tools_description)\n",
    "\n",
    "\n",
    "class KnowledgeItem:\n",
    "    description: str\n",
    "    tool: str\n",
    "\n",
    "    def __init__(self, description: str, tool: str, arg_mapping: tuple = None) -> None:\n",
    "        self.description = description\n",
    "        self.tool = tool\n",
    "        if arg_mapping:\n",
    "            self.arg_mapping = arg_mapping\n",
    "        else:\n",
    "            self.arg_mapping = ()\n",
    "\n",
    "    def summarize(self) -> str:\n",
    "        return self.description + \": \" + self.tool\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"Know <{self.description} from [{self.tool}]>\"\n",
    "\n",
    "    def __hash__(self) -> int:\n",
    "        return hash(str(self))\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return str(self)\n",
    "\n",
    "\n",
    "def get_base_knowledge(tools, arguments_description):\n",
    "    \"\"\"\n",
    "    Returns a list of knowledge items for all tools that don't have any arguments.\n",
    "    This is called the base knowledge as it is the starting point for the inference.\n",
    "    \"\"\"\n",
    "    knowledge = []\n",
    "\n",
    "    l = list(arguments_description.keys())\n",
    "\n",
    "    for tool in arguments_description:\n",
    "        if len(arguments_description[tool]) == 0:\n",
    "            tool_names = [t[\"name\"] for t in tools]\n",
    "            index = tool_names.index(tool)\n",
    "            tool_description = tools[index][\"description\"]\n",
    "            knowledge.append(KnowledgeItem(tool_description, tool))\n",
    "\n",
    "    return knowledge\n",
    "\n",
    "\n",
    "def elaborate_args(args: list[dict]):\n",
    "    response = \"\"\n",
    "    primary_count = 0\n",
    "    for arg in args:\n",
    "        for name, props in arg.items():\n",
    "            response += \"\\n- \"\n",
    "            response += f\"{name} ({props['type']}): {props['desc']}\"\n",
    "            primary_count += 1\n",
    "            if \"allowed\" in props.keys():\n",
    "                response += f\" allowing: {props['allowed']}\"\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_instruction_prompt(instruction, arguments_description, knowledge):\n",
    "    directive = instruction[0]\n",
    "    tool_to_be_used = instruction[1]\n",
    "\n",
    "    tool_arguments = arguments_description[tool_to_be_used]\n",
    "\n",
    "    prompt = \"Solve the 'Directive' with the given 'Tool'. Use values in 'Past Actions' or the provided directive and map values to arguments in the Tool. In case of missing info return a directive to get the missing info required to get missing info.\"\n",
    "    prompt += f\"\\nDirective:{directive}\\nTool: {tool_to_be_used} with args:{elaborate_args(tool_arguments)}\"\n",
    "\n",
    "    prompt += \"\\nPast Actions:\"\n",
    "    for knowledge_item in knowledge[::-1]:\n",
    "        prompt += f\"\\n- {knowledge_item.summarize()}\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_task(instructions: deque, tools, arguments_description, max_iter=10):\n",
    "    knowledge = get_base_knowledge(tools, arguments_description)\n",
    "\n",
    "    total_input_len = 0\n",
    "    total_output_len = 0\n",
    "    steps = 0\n",
    "    \n",
    "    if len(instructions) == 0:\n",
    "        return []\n",
    "\n",
    "    while len(instructions) > 0:\n",
    "        # sleep(0.5)\n",
    "        if steps > max_iter:\n",
    "            break\n",
    "\n",
    "        steps += 1\n",
    "        instruction = instructions[0]\n",
    "        response = palm.generate_text(\n",
    "            model=argument_mapping_model,\n",
    "            prompt=get_instruction_prompt(\n",
    "                instruction, arguments_description, knowledge\n",
    "            ),\n",
    "            temperature=0,\n",
    "            max_output_tokens=800,\n",
    "        )\n",
    "\n",
    "        total_input_len += len(\n",
    "            get_instruction_prompt(instruction, arguments_description, knowledge)\n",
    "        )\n",
    "\n",
    "        print(f\"Input: {instruction}\\nOutput: {response.result}\")\n",
    "\n",
    "        total_output_len += len(response.result)\n",
    "\n",
    "        response = eval(response.result)\n",
    "\n",
    "        if len(response.get(\"missing_action\", \"\")) > 0:\n",
    "            print(f\"Missing action: {response['missing_action']}\")\n",
    "\n",
    "            tool_for_missing_action = get_relevant_tools(\n",
    "                [response[\"missing_action\"]], tools, arguments_description\n",
    "            )\n",
    "\n",
    "            print(f\"Tool for missing action: {tool_for_missing_action}\")\n",
    "\n",
    "            if len(tool_for_missing_action) > 0:\n",
    "                instructions.appendleft(\n",
    "                    (response[\"missing_action\"], tool_for_missing_action[0][1])\n",
    "                )\n",
    "        else:\n",
    "            print(f\"Result: {response.get('result', [])}\")\n",
    "            instructions.popleft()\n",
    "            knowledge.append(\n",
    "                KnowledgeItem(instruction[0], instruction[1], response[\"result\"])\n",
    "            )\n",
    "\n",
    "    logging.debug(f\"Total tokens: {total_input_len}\")\n",
    "    return knowledge\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "def topo_sort(knowledge: list[KnowledgeItem], tools, arguments_description) -> list:\n",
    "    \"\"\"Returns a topologically sorted list of knowledge items.\"\"\"\n",
    "    if len(knowledge) == 0:\n",
    "        return []\n",
    "    \n",
    "    knowledge = deepcopy(knowledge)\n",
    "    final_goal = knowledge[-1]\n",
    "    for item in knowledge:\n",
    "        neighbors = set()\n",
    "        for arg in item.arg_mapping:\n",
    "            # print(arg)\n",
    "            if (\n",
    "                isinstance(arg[1], list)\n",
    "                or isinstance(arg[1], tuple)\n",
    "                or isinstance(arg[1], set)\n",
    "                or isinstance(arg[1], dict)\n",
    "            ):\n",
    "                nbs = [k_item for k_item in knowledge if k_item.tool in arg[1]]\n",
    "                neighbors.update(nbs)\n",
    "            else:\n",
    "                nbs = [k_item for k_item in knowledge if k_item.tool == arg[1]]\n",
    "                neighbors.update(nbs)\n",
    "        item.neighbors = neighbors\n",
    "\n",
    "    def topo_sort_util(k_item: KnowledgeItem, visited: set, stack: list):\n",
    "        visited.add(k_item)\n",
    "        for neighbor in k_item.neighbors:\n",
    "            if neighbor not in visited:\n",
    "                topo_sort_util(neighbor, visited, stack)\n",
    "        stack.append(k_item)\n",
    "\n",
    "    visited = set()\n",
    "    stack = []\n",
    "\n",
    "    topo_sort_util(final_goal, visited, stack)\n",
    "\n",
    "    # stack = stack[::-1]\n",
    "    solution = []\n",
    "    for item in stack:\n",
    "        tool_ordering = [k_item.tool for k_item in stack]\n",
    "        tool_args = [list(d.keys())[0] for d in  arguments_description[item.tool]]\n",
    "        solution_item = {}\n",
    "        solution_item[\"tool_name\"] = item.tool\n",
    "        solution_item[\"arguments\"] = []\n",
    "        for arg in item.arg_mapping:\n",
    "            if arg[0] not in tool_args:\n",
    "                continue\n",
    "            value = arg[1]\n",
    "            if isinstance(arg[1], list):\n",
    "                for i in range(len(arg[1])):\n",
    "                    if arg[1][i] in tool_ordering:\n",
    "                        value[i] = f\"$$PREV[{tool_ordering.index(arg[1][i])}]\"\n",
    "            elif arg[1] in tool_ordering:\n",
    "                value = f\"$$PREV[{tool_ordering.index(arg[1])}]\"\n",
    "\n",
    "            solution_item[\"arguments\"].append(\n",
    "                {\"argument_name\": arg[0], \"argument_value\": value}\n",
    "            )\n",
    "        solution.append(solution_item)\n",
    "\n",
    "    return solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the meaning of life?\n",
      "[]\n",
      "2.8881642818450928\n",
      "\n",
      "List all high severity tickets coming in from slack from customer Cust123 and generate a summary of them.\n",
      "Input: ('Get all high severity tickets coming in from slack from customer Cust123', 'works_list')\n",
      "Output: {'missing_action': 'Get all objects related to customer Cust123'}\n",
      "Missing action: Get all objects related to customer Cust123\n",
      "Tool for missing action: [('Get all objects related to customer Cust123', 'search_object_by_name')]\n",
      "Input: ('Get all objects related to customer Cust123', 'search_object_by_name')\n",
      "Output: {'result': [('query', 'Cust123')]}\n",
      "Result: [('query', 'Cust123')]\n",
      "Input: ('Get all high severity tickets coming in from slack from customer Cust123', 'works_list')\n",
      "Output: {'result': [('applies_to_part', 'search_object_by_name'), ('ticket.severity', 'high')]}\n",
      "Result: [('applies_to_part', 'search_object_by_name'), ('ticket.severity', 'high')]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tracemalloc import start\n",
    "\n",
    "\n",
    "class InferenceV1:\n",
    "    def __init__(self, tools, arg_cache=None):\n",
    "        self.tools = tools\n",
    "        self.argument_descriptions = generate_argument_descriptions(\n",
    "            self.tools, look_in=arg_cache\n",
    "        )\n",
    "\n",
    "    def invoke_agent(self, query):\n",
    "        task_segments = segement_task(query)\n",
    "\n",
    "        logging.debug(f\"Task segments: {task_segments}\")\n",
    "        task_and_tool = get_relevant_tools(\n",
    "            task_segments, self.tools, self.argument_descriptions\n",
    "        )\n",
    "\n",
    "        logging.debug(f\"Task and tool: {task_and_tool}\")\n",
    "\n",
    "        solution_knowledge = complete_task(\n",
    "            deque(task_and_tool), self.tools, self.argument_descriptions\n",
    "        )\n",
    "\n",
    "        final_solution = topo_sort(solution_knowledge, self.tools, self.argument_descriptions)\n",
    "\n",
    "        return final_solution\n",
    "\n",
    "\n",
    "arg_cache = cwd.joinpath(\"refined_arguments_description.json\")\n",
    "obj = InferenceV1(tools, arg_cache)\n",
    "examples = [\n",
    "    # \"Summarize work items similar to don:core:dvrv-us-1:devo/0:issue/1\",\n",
    "    \"What is the meaning of life?\",\n",
    "    # \"Prioritize my P0 issues and add them to the current sprint\",\n",
    "    # \"Summarize high severity tickets from the customer UltimateCustomer\",\n",
    "    # \"What are my all issues in the triage stage under part FEAT-123? Summarize them.\",\n",
    "    \"List all high severity tickets coming in from slack from customer Cust123 and generate a summary of them.\",\n",
    "    # \"Given a customer meeting transcript T, create action items and add them to my current sprint\",\n",
    "    # \"Get all work items similar to TKT-123, summarize them, create issues from that summary, and prioritize them\",\n",
    "]\n",
    "\n",
    "for example in examples:\n",
    "    print(example)\n",
    "    start = time.time()\n",
    "    print(json.dumps(obj.invoke_agent(example), indent=2))\n",
    "    print(time.time() - start)\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
