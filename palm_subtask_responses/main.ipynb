{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pratham\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import google.generativeai as palm\n",
    "\n",
    "my_config = json.load(open('config.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "palm.configure(api_key=my_config['palm_api_key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/text-bison-001\n"
     ]
    }
   ],
   "source": [
    "models = [m for m in palm.list_models() if 'generateText' in m.supported_generation_methods]\n",
    "model = models[0].name\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = \"models/embedding-gecko-001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "\n",
    "SCOPES = ['https://www.googleapis.com/auth/generative-language.tuning']\n",
    "\n",
    "def load_creds():\n",
    "    \"\"\"Converts `oauth-client-id.json` to a credential object.\n",
    "    \n",
    "    This function caches the generated tokens to minimize the use of the\n",
    "    consent screen.\n",
    "    \"\"\"\n",
    "    creds = None\n",
    "    # The file token.json stores the user's access and refresh tokens, and is\n",
    "    # created automatically when the authorization flow completes for the first\n",
    "    # time.\n",
    "    if os.path.exists('token.json'):\n",
    "        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'client_secret_56510766963.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.json', 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "    return creds\n",
    "\n",
    "# load_creds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_prompt = \"\"\"\n",
    "I have a argument for an function call. I want to summarize it and get its data type and possible values.\n",
    "For Example:\n",
    "\n",
    "Input:{\"name\": \"ticket.severity\",\"description\": \"Filters for tickets with any of the provided severities. Allowed values: blocker, high, low, medium\",\"type\": \"array of strings\"}\n",
    "Ouput: {\"desc\": \"Filters for tickets with any of the provided severities\", \"type\": \"Array[String]\", \"allowed\": [\"blocker\", \"high\", \"low\", \"medium\"]}\n",
    "\n",
    "Input:{\"name\": \"query\",\"description\": \"The search string, could be for example customer's name, part name, user name.\",\"type\": \"string\"}\n",
    "Output: {\"desc\": \"The search string, could be for example customer's name, part name, user name.\", \"type\": \"String\"}\n",
    "\n",
    "Input: {\"name\": \"limit\",\"description\": \"The maximum number of works to return. The default is '50'\",\"type\": \"integer (int32)\"}\n",
    "Output: {\"desc\": \"The maximum number of works to return.\", \"type\": \"int32\"}\n",
    "\n",
    "Input: {\"name\": \"applies_to_part\", \"description\": \"Filters for work belonging to any of the provided parts\", \"type\": \"array of strings\", \"example\": [ \"FEAT-123\", \"ENH-123\", \"PROD-123\", \"CAPL-123\" ]}\n",
    "Output: {\"desc\": \"Filters for work belonging to any of the provided parts.\", \"type\": \"Array[String]\", \"example\": [\"FEAT-123\", \"ENH-123\", \"PROD-123\", \"CAPL-123\"]}\n",
    "\n",
    "solve for the following argument ensuring output is valid json:\n",
    "Input: %s\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "tools = json.load(open('tools.json'))\n",
    "arguments_description = {}\n",
    "\n",
    "if os.path.exists('refined_arguments_description.json'):\n",
    "    arguments_description = json.load(open('refined_arguments_description.json'))\n",
    "else:\n",
    "    for tool in tools[\"tools\"]:\n",
    "        arguments_description[tool[\"name\"]] = []\n",
    "        for argument in tool.get(\"arguments\" ,[]):\n",
    "            output = palm.generate_text(\n",
    "                model=model,\n",
    "                prompt=arg_prompt % argument,\n",
    "                temperature=0,\n",
    "                max_output_tokens=800,\n",
    "            )\n",
    "            arguments_description[tool[\"name\"]].append({argument[\"name\"]: eval(output.result)}) \n",
    "    json.dump(arguments_description, open('refined_arguments_description.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_arguments_description = json.load(open('refined_arguments_description.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeItem:\n",
    "    description: str\n",
    "    tool: str\n",
    "\n",
    "    def __init__(self, description: str, tool: str) -> None:\n",
    "        self.description = description\n",
    "        self.tool = tool\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return f\"Know <{self.description} from [{self.tool}]>\"\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return str(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Know <the ID of the current sprint from [get_sprint_id]>, Know <the ID of the current user from [who_am_i]>]\n"
     ]
    }
   ],
   "source": [
    "knowledge = []\n",
    "\n",
    "l = list(refined_arguments_description.keys())\n",
    "\n",
    "for tool in refined_arguments_description:\n",
    "    if len(refined_arguments_description[tool]) == 0:\n",
    "        tool_names = [t['name'] for t in tools['tools']]\n",
    "        index = tool_names.index(tool)\n",
    "        tool_description = tools['tools'][index]['description'].split('Returns ')[1]\n",
    "        knowledge.append(KnowledgeItem(tool_description, tool))\n",
    "\n",
    "print(knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'applies_to_part': {'desc': 'Filters for work belonging to any of the '\n",
      "                              'provided parts.',\n",
      "                      'type': 'Array[String]',\n",
      "                      'example': ['FEAT-123',\n",
      "                                  'ENH-123',\n",
      "                                  'PROD-123',\n",
      "                                  'CAPL-123']}},\n",
      " {'created_by': {'desc': 'Filters for work created by any of these users',\n",
      "                 'type': 'Array[String]',\n",
      "                 'example': ['DEVU-123']}},\n",
      " {'owned_by': {'desc': 'Filters for work owned by any of these users',\n",
      "               'type': 'Array[String]',\n",
      "               'example': ['DEVU-123']}}]\n",
      "[{'objects': {'desc': 'List of objects to summarize',\n",
      "              'type': 'Array[Object]',\n",
      "              'example': [{'name': 'object1', 'type': 'object1'},\n",
      "                          {'name': 'object2', 'type': 'object2'}]}}]\n",
      "[{'objects': {'desc': 'A list of objects to be prioritized',\n",
      "              'type': 'Array[Object]',\n",
      "              'example': [{'id': '123456', 'type': 'work'}]}}]\n",
      "[{'work_ids': {'desc': 'A list of work item IDs to be added to the sprint.',\n",
      "               'type': 'Array[String]',\n",
      "               'example': ['123456', '654321']}},\n",
      " {'sprint_id': {'desc': 'The ID of the sprint to which the work items should '\n",
      "                        'be added',\n",
      "                'type': 'String',\n",
      "                'example': '123456'}}]\n",
      "[]\n",
      "[{'work_id': {'desc': 'The ID of the work item for which you want to find '\n",
      "                      'similar items',\n",
      "              'type': 'String',\n",
      "              'example': '1234567890'}}]\n",
      "[{'query': {'desc': 'The search string, could be for example customer’s name, '\n",
      "                    'part name, user name.',\n",
      "            'type': 'String',\n",
      "            'example': 'customer’s name, part name, user name.'}}]\n",
      "[{'text': {'desc': 'The text from which the actionable insights need to be '\n",
      "                   'created.',\n",
      "           'type': 'String',\n",
      "           'example': 'The text from which the actionable insights need to be '\n",
      "                      'created.'}}]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# tool_to_get = \"works_list\"\n",
    "for tool_to_get in tools['tools']:\n",
    "    tool_to_get = tool_to_get['name']\n",
    "    primary_arguments = refined_arguments_description[tool_to_get][:3]\n",
    "    pp(primary_arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "works_list:Returns a list of work items matching the request with args:\n",
      "\tapplies_to_part:Filters for work belonging to any of the provided parts\n",
      "summarize_objects:Summarizes a list of objects with args:\n",
      "\tobjects:List of objects to summarize\n",
      "prioritize_objects:Returns a list of objects sorted by priority with args:\n",
      "\tobjects:A list of objects to be prioritized\n",
      "add_work_items_to_sprint:Adds the given work items to the sprint with args:\n",
      "\twork_ids:A list of work item IDs to be added to the sprint\n",
      "get_sprint_id:Returns the ID of the current sprint\n",
      "get_similar_work_items:Returns a list of work items that are similar to the given work item with args:\n",
      "\twork_id:The ID of the work item for which you want to find similar items\n",
      "search_object_by_name:Given a search string, returns the id of a matching object in the system of record with args:\n",
      "\tquery:The search string, could be for example customer's name, part name, user name\n",
      "create_actionable_tasks_from_text:Given a text, extracts actionable insights, and creates tasks for them, which are kind of a work item with args:\n",
      "\ttext:The text from which the actionable insights need to be created\n",
      "who_am_i:Returns the ID of the current user\n",
      "1186\n"
     ]
    }
   ],
   "source": [
    "tools_description = \"\"\n",
    "\n",
    "for tool in tools[\"tools\"]:\n",
    "    tools_description += \"\\n\" + f\"{tool['name']}:{tool['description'].split('.')[0]}\" \n",
    "    for argument in refined_arguments_description[tool[\"name\"]][:1]:\n",
    "        tools_description += \" with args:\"\n",
    "        for arg, props in argument.items():\n",
    "            tools_description += f\"\\n\\t{arg}:{props['desc'].split('.')[0]}\"\n",
    "            # if 'example' in props.keys():\n",
    "            #     tools_description += f\" like: {props['example'][:2]}\"\n",
    "    \n",
    "\n",
    "\n",
    "print(tools_description)\n",
    "print(len(tools_description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_getter_prompt = \"\"\"\n",
    "You are a bot for a company to manage their work items using some tools\n",
    "Here are some actions you can perform with the tool:%s\n",
    "\n",
    "Using these as context you solve the relevant tasks strictly using these actions in the correct context as follows\n",
    "\n",
    "Input: List all high severity tickets coming in from slack from customer Cust123 and generate a summary of them\n",
    "Output: [(\"Get parts from customer Cust123\",\"search_object_by_name\"),(\"List all high severity tickets coming in from slack from them\",\"works_list\"),(\"Generate a summary of them\",\"summarize_objects\")]\n",
    "\n",
    "Input: Given a customer meeting transcript T, create action items and add them to my current sprint\n",
    "Output: [(\"Create action items from T\",\"create_actionable_tasks_from_text\"),(\"Add them to my current sprint\",\"add_tasks_to_sprint\")]\n",
    "\n",
    "Input: Prioritize my P0 issues and add them to the current sprint\n",
    "Output: [(\"Get my P0 issues\",\"works_list\"),(\"Prioritize them\",\"prioritize_objects\"),(\"Add them to the current sprint\",\"add_work_items_to_sprint\")]\n",
    "\n",
    "Input: What did the dog eat for breakfast?\n",
    "Output: []\n",
    "\n",
    "Give list for:\n",
    "Input: %s\n",
    "Output:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2383\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[(\"Get all work items similar to TKT-123\",\"get_similar_work_items\"),(\"Summarize them\",\"summarize_objects\"),(\"Create issues from that summary\",\"create_actionable_tasks_from_text\"),(\"Prioritize them\",\"prioritize_objects\")]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_prompt = \"\"\"What are my all issues in the triage stage under part FEAT-123? Summarize them.\"\"\"\n",
    "# '[(\"Get my issues in the triage stage under part FEAT-123\", \"works_list\"),(\"Summarize them\", \"summarize_objects\")]'\n",
    "\n",
    "input_prompt = \"\"\"Get all work items similar to TKT-123, summarize them, create issues from that summary, and prioritize them\"\"\"\n",
    "\n",
    "final_prompt = tool_getter_prompt % (tools_description, input_prompt)\n",
    "\n",
    "print(len(final_prompt))\n",
    "\n",
    "completion = palm.generate_text(\n",
    "    model=model,\n",
    "    prompt=final_prompt,\n",
    "    temperature=0,\n",
    "    # The maximum length of the response\n",
    "    max_output_tokens=800,\n",
    ")\n",
    "\n",
    "completion.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[(\"Get parts from customer Cust123\",\"search_object_by_name\"),(\"List all high severity tickets coming in from slack from them\",\"works_list\"),(\"Generate a summary of them\",\"summarize_objects\")]'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_prompt = \"\"\"List all high severity tickets coming in from slack from customer Cust123 and generate a summary of them.\"\"\"\n",
    "\n",
    "completion = palm.generate_text(\n",
    "    model=model,\n",
    "    prompt=tool_getter_prompt % (tools_description,input_prompt),\n",
    "    temperature=0,\n",
    "    max_output_tokens=800,\n",
    ")\n",
    "\n",
    "completion.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = eval(completion.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(embeddings1, embeddings2):\n",
    "    vector1 = np.array(embeddings1)\n",
    "    vector2 = np.array(embeddings2)\n",
    "\n",
    "    # Calculate dot product\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "\n",
    "    # Calculate magnitudes\n",
    "    magnitude1 = np.linalg.norm(vector1)\n",
    "    magnitude2 = np.linalg.norm(vector2)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity = dot_product / (magnitude1 * magnitude2)\n",
    "\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palm.generate_text(\n",
    "    model=embeddings_model,\n",
    "    prompt=\"Get all work items similar to TKT-123, summarize them, create issues from that summary, and prioritize them\",\n",
    "    temperature=0,\n",
    "    max_output_tokens=800,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We want to do:List all high severity tickets coming in from slack from them, using the tool works_list with the following arguments:\n",
      "\tapplies_to_part:Filters for work belonging to any of the provided parts.\n",
      "\tcreated_by:Filters for work created by any of these users\n",
      "\towned_by:Filters for work owned by any of these users\n",
      "\tissue.rev_orgs:Filters for issues with any of the provided Rev organizations\n",
      "\tissue.priority:Filters for issues with any of the provided priorities allowing: ['p0', 'p1', 'p2', 'p3']\n",
      "\tlimit:The maximum number of works to return.\n",
      "\tstage.name:Filters for records in the provided stage(s) by name\n",
      "\tticket.needs_response:Filters for tickets that need a response\n",
      "\tticket.severity:Filters for tickets with any of the provided severities allowing: ['blocker', 'high', 'low', 'medium']\n",
      "\tticket.source_channel:Filters for tickets with any of the provided source channels\n",
      "\ttypes:Filters for work of the provided types. allowing: ['issue', 'ticket', 'task']\n",
      "\t\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "def elaborate_args(args: list[dict]):\n",
    "    response = \"\"\n",
    "    for arg in args:\n",
    "        for name, props in arg.items():\n",
    "            response += f\"{name}:{props['desc']}\"\n",
    "            if 'allowed' in props.keys():\n",
    "                response += f\" allowing: {props['allowed']}\"\n",
    "            response += \"\\n\\t\"\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_tool_arguments(instruction):\n",
    "    directive = instruction[0]\n",
    "    tool_to_be_used = instruction[1]\n",
    "\n",
    "    tool_arguments = refined_arguments_description[tool_to_be_used]\n",
    "\n",
    "    prompt = f\"\"\"We want to do:{directive}, using the tool {tool_to_be_used} with the following arguments:\\n\\t{elaborate_args(tool_arguments)}\n",
    "    \"\"\"\n",
    "\n",
    "    print(prompt)\n",
    "\n",
    "get_tool_arguments(instructions[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argument_embeddings = json.load(open('argument_embeddings.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query 0.6966728323092024\n"
     ]
    }
   ],
   "source": [
    "directive_embeddings = palm.generate_embeddings(\n",
    "    model=embeddings_model,\n",
    "    text=\"Get parts from customer Cust123\",\n",
    ")\n",
    "\n",
    "for arg in argument_embeddings['search_object_by_name']:\n",
    "    arg_embedding = argument_embeddings['search_object_by_name'][arg]\n",
    "    print(arg, cosine_similarity(arg_embedding, directive_embeddings['embedding']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "doc = nlp(\"Get parts from Cust123\")\n",
    "\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[parts, Cust123]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(doc.noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List ROOT List NOUN [tickets]\n",
      "all det tickets NOUN []\n",
      "high amod tickets NOUN []\n",
      "severity compound tickets NOUN []\n",
      "tickets dobj List NOUN [all, high, severity, from]\n",
      "from prep tickets NOUN [them]\n",
      "them pobj from ADP []\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.doc.Doc' object has no attribute 'adjectives'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Pratham\\Desktop\\dir\\InterIIT\\DevRev\\devrev-ai-agents\\palm_subtask_responses\\main.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Pratham/Desktop/dir/InterIIT/DevRev/devrev-ai-agents/palm_subtask_responses/main.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m chunks:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Pratham/Desktop/dir/InterIIT/DevRev/devrev-ai-agents/palm_subtask_responses/main.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     cat_text \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m chunk\u001b[39m.\u001b[39mtext \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Pratham/Desktop/dir/InterIIT/DevRev/devrev-ai-agents/palm_subtask_responses/main.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(doc2\u001b[39m.\u001b[39;49madjectives)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'spacy.tokens.doc.Doc' object has no attribute 'adjectives'"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(\"List all high severity tickets from them\")\n",
    "\n",
    "for token in doc2:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "            [child for child in token.children])\n",
    "chunks = list(doc2.noun_chunks)\n",
    "cat_text = \"\"\n",
    "for chunk in chunks:\n",
    "    cat_text += chunk.text + \" \"\n",
    "\n",
    "print(doc2.adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(map(list(doc.noun_chunks), str())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = refined_arguments_description['works_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(palm.list_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "argument_embeddings = {}\n",
    "x = None\n",
    "for tool in refined_arguments_description:\n",
    "    argument_embeddings[tool] = {}\n",
    "    for argument in refined_arguments_description[tool]:\n",
    "        for arg, props in argument.items():\n",
    "            argument_ = arg + \":\" + props['desc']\n",
    "            # print(argument_)\n",
    "            embeddings = palm.generate_embeddings(\n",
    "                model=\"models/embedding-gecko-001\",\n",
    "                text=argument_\n",
    "            )\n",
    "            argument_embeddings[tool][arg] = embeddings['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(argument_embeddings, open('argument_embeddings.json', 'w'), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_embeddings = palm.generate_embeddings(\n",
    "    model=\"models/embedding-gecko-001\",\n",
    "    text=\"List all high severity tickets them \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(embeddings1, embeddings2):\n",
    "    vector1 = np.array(embeddings1)\n",
    "    vector2 = np.array(embeddings2)\n",
    "\n",
    "    # Calculate dot product\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "\n",
    "    # Calculate magnitudes\n",
    "    magnitude1 = np.linalg.norm(vector1)\n",
    "    magnitude2 = np.linalg.norm(vector2)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity = dot_product / (magnitude1 * magnitude2)\n",
    "\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applies_to_part 0.6344296627448915\n",
      "created_by 0.5870074218989871\n",
      "owned_by 0.6118433946207997\n",
      "issue.rev_orgs 0.660691662266941\n",
      "issue.priority 0.7029635073056274\n",
      "limit 0.6032555833625599\n",
      "stage.name 0.5941191389196038\n",
      "ticket.needs_response 0.7088068220806242\n",
      "ticket.severity 0.8144742037308171\n",
      "ticket.source_channel 0.6731756104675004\n",
      "types 0.6195709153384679\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "\n",
    "for arg in argument_embeddings['works_list']:\n",
    "    arg_embedding = argument_embeddings['works_list'][arg]\n",
    "    print(arg, cosine_similarity(arg_embedding, info_embeddings['embedding']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List all high severity tickets from them\n",
      "applies_to_part : Filters for work belonging to any of the provided parts.\n",
      "[(all high severity tickets, 0.6258663710735963), (them, 0.5119058029487383)]\n",
      "\n",
      "created_by : Filters for work created by any of these users\n",
      "[(all high severity tickets, 0.5973284370231899), (them, 0.44365882280241603)]\n",
      "\n",
      "owned_by : Filters for work owned by any of these users\n",
      "[(all high severity tickets, 0.5853890601086897), (them, 0.42142880469493155)]\n",
      "\n",
      "issue.rev_orgs : Filters for issues with any of the provided Rev organizations\n",
      "[(all high severity tickets, 0.5996469246404537), (them, 0.37242701507888104)]\n",
      "\n",
      "issue.priority : Filters for issues with any of the provided priorities\n",
      "[(all high severity tickets, 0.6185587946715899), (them, 0.39029457688801417)]\n",
      "\n",
      "limit : The maximum number of works to return.\n",
      "[(all high severity tickets, 0.5937899029368208), (them, 0.4160570717768777)]\n",
      "\n",
      "stage.name : Filters for records in the provided stage(s) by name\n",
      "[(all high severity tickets, 0.3557460359282059)]\n",
      "\n",
      "ticket.needs_response : Filters for tickets that need a response\n",
      "[(all high severity tickets, 0.6120306726063237), (them, 0.5126315493948626)]\n",
      "\n",
      "ticket.severity : Filters for tickets with any of the provided severities\n",
      "[(all high severity tickets, 0.6569149834043715), (them, 0.3888706068153)]\n",
      "\n",
      "ticket.source_channel : Filters for tickets with any of the provided source channels\n",
      "[(all high severity tickets, 0.6479007073805757), (them, 0.39455192141321166)]\n",
      "\n",
      "types : Filters for work of the provided types.\n",
      "[(all high severity tickets, 0.5967596566319996), (them, 0.3681279775055831)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(doc2)\n",
    "for arg in args:\n",
    "    for name, props in arg.items():\n",
    "        print(f\"{name} : {props['desc']}\")\n",
    "        relevant_noun_chunks = [(chunk, chunk.similarity(nlp(props['desc']))) for chunk in doc2.noun_chunks if chunk.similarity(nlp(props['desc'])) > 0.3]\n",
    "        print(relevant_noun_chunks)\n",
    "\n",
    "        # print(doc2.noun_chunks[0].similarity(nlp(props['desc'])))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filters for work belonging to any of the provided parts. 0.6678973009450858\n",
      "Filters for work created by any of these users. 0.6391649774712338\n",
      "Filters for work owned by any of these users. 0.6248281978477502\n",
      "Filters for issues with any of the provided Rev organizations. 0.6137762123198153\n",
      "Filters for issues with any of the provided priorities. 0.6312085020893078\n",
      "The maximum number of works to return. 0.6117348495352216\n",
      "Filters for records in the provided stage(s) by name. 0.366006611664165\n",
      "Filters for tickets that need a response. 0.6691003652021462\n",
      "Filters for tickets with any of the provided severities. 0.6597274816160735\n",
      "Filters for tickets with any of the provided source channels. 0.6549307538800886\n",
      "Filters for work of the provided types. 0.5990385488555882\n",
      "The most relevant input field for 'all high severity tickets them' is: ticket.needs_response\n"
     ]
    }
   ],
   "source": [
    "input_fields = {\n",
    "    'applies_to_part': 'Filters for work belonging to any of the provided parts.',\n",
    "    'created_by': 'Filters for work created by any of these users.',\n",
    "    'owned_by': 'Filters for work owned by any of these users.',\n",
    "    'issue.rev_orgs': 'Filters for issues with any of the provided Rev organizations.',\n",
    "    'issue.priority': 'Filters for issues with any of the provided priorities.',\n",
    "    'limit': 'The maximum number of works to return.',\n",
    "    'stage.name': 'Filters for records in the provided stage(s) by name.',\n",
    "    'ticket.needs_response': 'Filters for tickets that need a response.',\n",
    "    'ticket.severity': 'Filters for tickets with any of the provided severities.',\n",
    "    'ticket.source_channel': 'Filters for tickets with any of the provided source channels.',\n",
    "    'types': 'Filters for work of the provided types.'\n",
    "}\n",
    "\n",
    "# Given noun chunk\n",
    "noun_chunk = \"all high severity tickets them\"\n",
    "\n",
    "# Process the noun chunk with spaCy\n",
    "noun_doc = nlp(noun_chunk)\n",
    "\n",
    "# Define a function to calculate similarity between the noun chunk and each input field description\n",
    "def calculate_similarity(description):\n",
    "    x = noun_doc.similarity(nlp(description))\n",
    "    print(description, x)\n",
    "    return x\n",
    "\n",
    "# Find the most relevant input field\n",
    "most_relevant_field = max(input_fields, key=lambda field: calculate_similarity(input_fields[field]))\n",
    "\n",
    "# Print the result\n",
    "print(f\"The most relevant input field for '{noun_chunk}' is: {most_relevant_field}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Filters, issues, any, the provided Rev organizations]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = nlp('Filters for issues with any of the provided Rev organizations.')\n",
    "list(d.noun_chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
